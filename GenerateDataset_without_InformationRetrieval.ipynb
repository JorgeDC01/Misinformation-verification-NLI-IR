{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "A9xMP0buE92M",
        "wkPCME23TRsr",
        "ncEqhiwNQO7C"
      ],
      "authorship_tag": "ABX9TyOfnvCjqqpumtp7q1kVLDmB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Primera aproximación. Generacion del dataset [evidences + distractors] sin Information Retrieval.\n",
        "\n",
        "En este script, se generan los conjuntos de datos de entrenamiento y validación, combinando los *embeddings* de los *claims* junto con los *embeddings* de las *wiki-pages* en un mismo archivo `.jsonl`.  \n",
        "\n",
        "- El apartado **Por Fases** crea primero el dataset de evidencias y, sobre él, se insertan los distractores. Este enfoque permite un mejor manejo del proceso seguido para elegir los distractores.  \n",
        "- El apartado **Completo** genera el dataset directamente con las evidencias y los distractores.  \n",
        "\n",
        "Como resultado, se obtienen los archivos `e_train_LoRA` y `e_val_LoRA`, que contienen los *embeddings* de las evidencias de cada *claim*.  \n",
        "\n",
        "El archivo `e_val_LoRA` incluye tanto las muestras de validación como las de prueba. Una vez generado, se divide en `val` y `test`.  \n",
        "\n",
        "El siguiente paso es incluir *embeddings* de **detractores**, es decir, documentos que no son relevantes para la verificación del *claim*.  \n",
        "\n"
      ],
      "metadata": {
        "id": "93mb33hDCRiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets --quiet\n",
        "!python -m pip install ujson --quiet"
      ],
      "metadata": {
        "id": "lMZouiUaEyJT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741887862141,
          "user_tz": -60,
          "elapsed": 8601,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d237b7a-9a13-4059-def6-709ac68efb93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/485.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m389.1/485.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "cache_dir = \"/root/.cache/huggingface/datasets\"  # Directorio de cache en Colab\n",
        "\n",
        "# Elimina el directorio de cache\n",
        "shutil.rmtree(cache_dir)"
      ],
      "metadata": {
        "id": "QnKQ_HVHl8MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Librerías y código extra, necesario para la ejecución completa del cuaderno\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Para cargar datasets del HUB de HuggingFace\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset, DownloadConfig\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "import gc\n",
        "from huggingface_hub import HfApi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M0jMOTJE2TL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741887899413,
          "user_tz": -60,
          "elapsed": 37253,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "347c5e5a-ae5a-4e8f-8156-56b5d7269fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Por fases**\n",
        "1. Fase: Generacion Lora Dataset con evidencias\n",
        "2. Fase: Generacion Lora Dataset con evidencias y distractores\n"
      ],
      "metadata": {
        "id": "DQpkoQSBHmA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1ª Fase: Combinación de los embeddings de las evidencias junto con el de los claims**"
      ],
      "metadata": {
        "id": "A9xMP0buE92M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import list_datasets\n",
        "\n",
        "datasets = list_datasets(author=\"JORGEDC01\")\n",
        "print([d.id for d in datasets])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r79x1kL0E30y",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741862430916,
          "user_tz": -60,
          "elapsed": 848,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "c8984997-0114-4e7a-fed5-159b164bccb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['JORGEDC01/Demo1']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"train\": \"e_train.jsonl\", \"val\": \"e_val.jsonl\"}\n",
        "download_config = DownloadConfig(disable_tqdm=True)\n",
        "dataset = load_dataset(\"JORGEDC01/Demo1\", data_dir=\"Data/Embeddings_Data/bge_small_en_v1.5\", data_files=data_files, download_config=download_config) # oontiene los datos de claims de FEVER\n",
        "print(dataset)\n",
        "\n",
        "#train_dataset = dataset['train']\n",
        "#val_dataset = dataset['val']\n",
        "\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "df_val = pd.DataFrame(dataset['val'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "1211f97155344f288fa0888bb7b0544a",
            "d262dcf875d0486c85d65454d64f6770",
            "aafc3524ca1d4ea6ae029057bea19d6d",
            "bd4b9a05fdc346699f2f5fd38d5d0184",
            "2fa6255454a340ca9631f25a2d29d219",
            "9dfafa31a7cb45e9b4e731c19ec4d984",
            "4da947f46c034406844911ea5f7048e4",
            "85568590edeb471f995940c6edc10ffc",
            "1764c7a64f2946f6bddb9b33403520c8",
            "9938ae2d036046c9ac22e05169c00452",
            "bfda811279244e73ad0185a5db443322",
            "0b18165f81274bbeb3980e622a1c5177",
            "89807b125b16444285e768ea0a6338bc",
            "f696ad28207b4092b9358f20f5eb9529",
            "7e073f7a2c424478b3f0d3cd5332d43c",
            "406d407cb5764e668c1c2863816bedd2",
            "03b61f25e3ca4543b4ee3cf930b54674",
            "6339dc5c61ca4b878ba2664766379a1b",
            "2fc451092eb84a3a961568bf36c1c6ed",
            "d389009f23e543b2bc62d0463ef38940",
            "c5ed1424ac80465ab7e8dccecae920cc",
            "6cd75ef957024f3a9aa4cc1d05a00451",
            "d25666b3d4d743bf831aca1694af1849",
            "14ddc41f5a034772a376ca049866490b",
            "590aa94d5e6b44f5a00e5310efccba6a",
            "fc6cafaa5b9c4be89eb54045e07340ab",
            "7b01debe47384118bf57600068e91356",
            "bf1a7d57411c474dbc85c6454e1997e8",
            "b590a28c78af49668e86a65613dd9cf8",
            "c4f47eeb34af4041ba34ae1d7ecb26be",
            "226d8cb369f64dd694b8eacc0c06c821",
            "3b7f4e5a80b6469fbf6b2bf01fbc7221",
            "f8065a04b2674a028ee6b1ff9f5aa88f",
            "6e94c886a8e145b19dfa91aaff11fab9",
            "d2710e6a2a6e47219c7faf9b1429b07f",
            "f10ae10b7eaf440f81db47baab12cc4c",
            "97002144c89249d1896447eeb3492065",
            "1deeed43206b4959b6e5a9069a39068f",
            "bbccd174554c4ebd94f803950f55355a",
            "027b4718dcf5432dac9daa645a3dc221",
            "44664ab120454d8e88757bb230be5510",
            "87bfe2537e724307945b2d77f0dc50e7",
            "c52d47c882df473c96ab1b351064e87b",
            "44e4c37f7cb24e249cb063f156c9a0c9",
            "52fc0499a0b54aa686c82981184dbff7",
            "56e5d4f050404ae1a384709356dce586",
            "07c8e45b1c9a4494bf4d6b59670132da",
            "f3acdf0bcc714427b4c41fa028a5c94e",
            "92fae8a7d8444bd8ab46072069326d48",
            "b0fbf1cfc6e347aeb25453c240d42323",
            "04b08ae03e224f908a14e7f6b8173148",
            "ebc89b7746bf48588cc59ca81990d882",
            "7405fde62bd84c00a6248d72455a00bf",
            "61dfbdbda51849199cb151486a3e2853",
            "18707c74049b4cbab827a86ebf9c62b0"
          ]
        },
        "id": "M9aHPI1ZFjJr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741862563818,
          "user_tz": -60,
          "elapsed": 87530,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "a5f23279-8421-4228-be8d-aebcc9ade6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1211f97155344f288fa0888bb7b0544a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "e_train.jsonl:   0%|          | 0.00/777M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b18165f81274bbeb3980e622a1c5177"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "e_val.jsonl:   0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d25666b3d4d743bf831aca1694af1849"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e94c886a8e145b19dfa91aaff11fab9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating val split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52fc0499a0b54aa686c82981184dbff7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'verifiable', 'label', 'claim', 'evidence', 'claim_embedding'],\n",
            "        num_rows: 145449\n",
            "    })\n",
            "    val: Dataset({\n",
            "        features: ['id', 'verifiable', 'label', 'claim', 'evidence', 'claim_embedding'],\n",
            "        num_rows: 19998\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del dataset\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNy9IoeYT4Nj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741862568262,
          "user_tz": -60,
          "elapsed": 6,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "c1de925d-9e1c-4ec8-ce80-fe81745ba647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "pEltVPyU5Uc6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741862568284,
          "user_tz": -60,
          "elapsed": 20,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "471efe0c-586f-4905-a997-c79b240b5459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id  verifiable     label  \\\n",
              "0   75397  VERIFIABLE  SUPPORTS   \n",
              "1  150448  VERIFIABLE  SUPPORTS   \n",
              "\n",
              "                                               claim  \\\n",
              "0  Nikolaj Coster-Waldau worked with the Fox Broa...   \n",
              "1                 Roman Atwood is a content creator.   \n",
              "\n",
              "                                            evidence  \\\n",
              "0  [[92206, 104971, Nikolaj_Coster-Waldau, 7], [9...   \n",
              "1                [[174271, 187498, Roman_Atwood, 1]]   \n",
              "\n",
              "                                     claim_embedding  \n",
              "0  [-0.0048766928, 0.012141332, -0.0167379342, -0...  \n",
              "1  [-0.0606420673, 0.0680666119, 0.040761821, -0....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c620c4f-b761-4694-b3e8-7e8a285a7bb4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>verifiable</th>\n",
              "      <th>label</th>\n",
              "      <th>claim</th>\n",
              "      <th>evidence</th>\n",
              "      <th>claim_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75397</td>\n",
              "      <td>VERIFIABLE</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>Nikolaj Coster-Waldau worked with the Fox Broa...</td>\n",
              "      <td>[[92206, 104971, Nikolaj_Coster-Waldau, 7], [9...</td>\n",
              "      <td>[-0.0048766928, 0.012141332, -0.0167379342, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>150448</td>\n",
              "      <td>VERIFIABLE</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>Roman Atwood is a content creator.</td>\n",
              "      <td>[[174271, 187498, Roman_Atwood, 1]]</td>\n",
              "      <td>[-0.0606420673, 0.0680666119, 0.040761821, -0....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c620c4f-b761-4694-b3e8-7e8a285a7bb4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c620c4f-b761-4694-b3e8-7e8a285a7bb4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c620c4f-b761-4694-b3e8-7e8a285a7bb4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d2959c1c-72f8-4f7d-aa78-9c60c7f358f3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2959c1c-72f8-4f7d-aa78-9c60c7f358f3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d2959c1c-72f8-4f7d-aa78-9c60c7f358f3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> La siguiente celda reduce los conjuntos. *Se ha utilizado 0.1 y 0.3 en el train*. Se usa el dataset de validación completo. Una vez generado, se separa en test y validación (del mismo modo que en el paper)."
      ],
      "metadata": {
        "id": "Pnc82F_fBMWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_small, _ = train_test_split(df_train, train_size=0.35, stratify=df_train[\"label\"], random_state=42).copy()\n",
        "val_small = df_val.copy()\n",
        "\n",
        "print(f\"Tamaño original train: {df_train.shape}, Tamaño reducido: {train_small.shape}\")\n",
        "print(f\"Tamaño original val: {df_val.shape}, Tamaño reducido: {val_small.shape}\")\n",
        "\n",
        "# dataset_embeddings = load_dataset(\"JORGEDC01/Demo1\", data_dir=\"wiki-pages/Embeddings_WikiPages/bge_small_en_v1.5/jsonl\", streaming=True) # oontiene los datos de claims de FEVER\n",
        "# iterable_ds_embeddings = dataset_embeddings[\"train\"]\n",
        "\n",
        "# for wikipage in iterable_ds_embeddings:\n",
        "#     print(wikipage)\n",
        "#     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFIeH5d15TCV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741862594304,
          "user_tz": -60,
          "elapsed": 618,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "6e7c95d0-0978-464a-f896-6666e489307d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño original train: (145449, 6), Tamaño reducido: (50907, 6)\n",
            "Tamaño original val: (19998, 6), Tamaño reducido: (19998, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_small.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "aSSPqiO65Bv-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741862598729,
          "user_tz": -60,
          "elapsed": 327,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "2fdf68e3-021d-4083-83d3-f8b49cb99135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id  verifiable     label  \\\n",
              "27207   94258  VERIFIABLE   REFUTES   \n",
              "89833  106413  VERIFIABLE  SUPPORTS   \n",
              "\n",
              "                                                   claim  \\\n",
              "27207                          Haitian Creole is a book.   \n",
              "89833  Loving tells the story of the 1967 U.S. Suprem...   \n",
              "\n",
              "                                                evidence  \\\n",
              "27207              [[111682, 125488, Haitian_Creole, 0]]   \n",
              "89833  [[124915, 139246, Loving_-LRB-2016_film-RRB-, 0]]   \n",
              "\n",
              "                                         claim_embedding  \n",
              "27207  [-0.0263465252, 0.0131574152, 0.0542918034, -0...  \n",
              "89833  [-0.0248791277, 0.0372825973, -0.0372912735, -...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84027b2c-a3ff-4063-9ada-c37597214cc0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>verifiable</th>\n",
              "      <th>label</th>\n",
              "      <th>claim</th>\n",
              "      <th>evidence</th>\n",
              "      <th>claim_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27207</th>\n",
              "      <td>94258</td>\n",
              "      <td>VERIFIABLE</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>Haitian Creole is a book.</td>\n",
              "      <td>[[111682, 125488, Haitian_Creole, 0]]</td>\n",
              "      <td>[-0.0263465252, 0.0131574152, 0.0542918034, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89833</th>\n",
              "      <td>106413</td>\n",
              "      <td>VERIFIABLE</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>Loving tells the story of the 1967 U.S. Suprem...</td>\n",
              "      <td>[[124915, 139246, Loving_-LRB-2016_film-RRB-, 0]]</td>\n",
              "      <td>[-0.0248791277, 0.0372825973, -0.0372912735, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84027b2c-a3ff-4063-9ada-c37597214cc0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84027b2c-a3ff-4063-9ada-c37597214cc0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84027b2c-a3ff-4063-9ada-c37597214cc0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d05ec6e0-2d91-4317-8ca9-8504d63b6abc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d05ec6e0-2d91-4317-8ca9-8504d63b6abc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d05ec6e0-2d91-4317-8ca9-8504d63b6abc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_small",
              "summary": "{\n  \"name\": \"train_small\",\n  \"rows\": 50907,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66494,\n        \"min\": 7,\n        \"max\": 229447,\n        \"num_unique_values\": 50907,\n        \"samples\": [\n          50947,\n          177625,\n          1965\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"verifiable\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NOT VERIFIABLE\",\n          \"VERIFIABLE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"REFUTES\",\n          \"SUPPORTS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"claim\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49136,\n        \"samples\": [\n          \"BitTorrent has been used by Radiohead.\",\n          \"Boston Latin School is the youngest school in the United States.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evidence\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"claim_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Train**"
      ],
      "metadata": {
        "id": "kSqD7WOPlyeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import ujson\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from huggingface_hub import hf_hub_download\n",
        "import unicodedata\n",
        "\n",
        "# --- Hugging Face ---\n",
        "repo_id = \"JORGEDC01/Demo1\"\n",
        "subfolder = \"wiki-pages/Embeddings_WikiPages/bge_small_en_v1.5/jsonl/\"\n",
        "jsonl_files = [f\"wiki-{i:03}.jsonl\" for i in range(1, 101)]  # Desde 001 hasta 100\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    return unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "\n",
        "# --- Indice/Diccionario de evidencias ---\n",
        "evidence_dict = {}\n",
        "for _, row in train_small.iterrows():\n",
        "    for ev in row[\"evidence\"]:\n",
        "        if isinstance(ev, list) and len(ev) >= 3:\n",
        "            # ev_id = ev[2]  # ID de la evidencia\n",
        "            ev_id = normalize_text(ev[2])\n",
        "\n",
        "            if ev_id == \"None\":\n",
        "                continue\n",
        "\n",
        "            if ev_id not in evidence_dict:\n",
        "                evidence_dict[ev_id] = []\n",
        "            evidence_dict[ev_id].append(ev)\n",
        "\n",
        "\n",
        "# --- Función para procesar cada archivo JSONL ---\n",
        "def process_jsonl(file_name):\n",
        "    results = {}\n",
        "\n",
        "    try:\n",
        "        file_path = hf_hub_download(repo_id=repo_id, filename=f\"{subfolder}{file_name}\", repo_type='dataset')\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                data = ujson.loads(line)\n",
        "                # wiki_id = data[\"id\"]\n",
        "                wiki_id = normalize_text(data[\"id\"])\n",
        "\n",
        "                if wiki_id in evidence_dict:\n",
        "                    for ev in evidence_dict[wiki_id]:\n",
        "                        ev.append(data[\"text_embedding\"])  # Se añade el embedding en el indice\n",
        "\n",
        "                    results[wiki_id] = evidence_dict[wiki_id]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {file_name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Procesamiento en paralelo de los 100 archivos  ---\n",
        "final_results = {}\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=1) as executor:  # Procesa dos archivos simultáneamente\n",
        "    for partial_results in tqdm(executor.map(process_jsonl, jsonl_files), total=len(jsonl_files)):\n",
        "        for key, value in partial_results.items():\n",
        "            if key in final_results:\n",
        "                final_results[key].extend(value)  # Evita sobrescribir, combina embeddings\n",
        "            else:\n",
        "                final_results[key] = value\n",
        "\n",
        "# --- Asignar los embeddings en el DataFrame ---\n",
        "for idx, row in train_small.iterrows():\n",
        "    updated_evidence = []\n",
        "    for ev in row[\"evidence\"]:\n",
        "        if isinstance(ev, list) and len(ev) >= 3:\n",
        "            # ev_id = ev[2]\n",
        "            ev_id = normalize_text(ev[2])\n",
        "\n",
        "            # Si la evidencia tiene \"None\" en los campos importantes, se mantiene igual\n",
        "            if ev_id == \"None\":\n",
        "                updated_evidence.append(ev)\n",
        "                continue\n",
        "\n",
        "            # if ev_id in final_results:\n",
        "            #     updated_evidence.extend(final_results[ev_id])\n",
        "\n",
        "            if ev_id in final_results:\n",
        "              for stored_ev in final_results[ev_id]:\n",
        "                  if stored_ev[:3] == ev[:3]:  # Verifica que sean exactamente la misma evidencia\n",
        "                      updated_evidence.append(stored_ev)\n",
        "                      break  # Como ya encontramos la evidencia correcta, no necesitamos seguir buscando\n",
        "\n",
        "            else:\n",
        "                updated_evidence.append(ev)  # mantiene la evidencia sin cambios si no tiene embedding\n",
        "\n",
        "    train_small.at[idx, \"evidence\"] = updated_evidence  # Modifica la fila directamente\n",
        "\n",
        "# --- Guardar en JSONL ---\n",
        "output_file = \"e_train_LoRA_50k.jsonl\"\n",
        "\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in train_small.iterrows():\n",
        "        output_data = {}\n",
        "\n",
        "        for field in [\"verifiable\", \"label\", \"claim\", \"claim_embedding\"]:\n",
        "            if field in row:\n",
        "                output_data[field] = row[field]\n",
        "\n",
        "        output_data[\"evidence\"] = []\n",
        "        for ev in row[\"evidence\"]:\n",
        "            if isinstance(ev, list) and len(ev) > 3:\n",
        "                embedding = ev[-1] if isinstance(ev[-1], list) else []\n",
        "                output_data[\"evidence\"].append(embedding)\n",
        "\n",
        "        # Si todas las evidencias son listas vacías, deja solo una lista vacía global\n",
        "        if all(isinstance(ev, list) and not ev for ev in output_data[\"evidence\"]):\n",
        "            output_data[\"evidence\"] = []\n",
        "\n",
        "        f.write(ujson.dumps(output_data) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Archivo guardado con formato limpio: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QWw8omlvfXz",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741863254179,
          "user_tz": -60,
          "elapsed": 643005,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "9221bab8-5eb4-46ea-abcc-eb21d5ea343b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [10:20<00:00,  6.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo guardado con formato limpio: e_train_LoRA_50k.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Development**"
      ],
      "metadata": {
        "id": "Hd15do8Nl1Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import ujson\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from huggingface_hub import hf_hub_download\n",
        "import unicodedata\n",
        "\n",
        "# --- Hugging Face ---\n",
        "repo_id = \"JORGEDC01/Demo1\"\n",
        "subfolder = \"wiki-pages/Embeddings_WikiPages/bge_small_en_v1.5/jsonl/\"\n",
        "jsonl_files = [f\"wiki-{i:03}.jsonl\" for i in range(1, 101)]  # Desde 001 hasta 100\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    return unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "\n",
        "# --- Indice/Diccionario de evidencias ---\n",
        "evidence_dict = {}\n",
        "for _, row in val_small.iterrows():\n",
        "    for ev in row[\"evidence\"]:\n",
        "        if isinstance(ev, list) and len(ev) >= 3:\n",
        "            ev_id = normalize_text(ev[2])\n",
        "\n",
        "            if ev_id == \"None\":\n",
        "                continue\n",
        "\n",
        "            if ev_id not in evidence_dict:\n",
        "                evidence_dict[ev_id] = []\n",
        "            evidence_dict[ev_id].append(ev)\n",
        "\n",
        "\n",
        "# --- Función para procesar cada archivo JSONL ---\n",
        "def process_jsonl(file_name):\n",
        "    results = {}\n",
        "\n",
        "    try:\n",
        "        file_path = hf_hub_download(repo_id=repo_id, filename=f\"{subfolder}{file_name}\", repo_type='dataset')\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                data = ujson.loads(line)\n",
        "                wiki_id = normalize_text(data[\"id\"])\n",
        "\n",
        "                if wiki_id in evidence_dict:\n",
        "                    for ev in evidence_dict[wiki_id]:\n",
        "                        ev.append(data[\"text_embedding\"])  # Se añade el embedding en el indice\n",
        "\n",
        "                    results[wiki_id] = evidence_dict[wiki_id]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {file_name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Procesamiento en paralelo de los 100 archivos  ---\n",
        "final_results = {}\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=1) as executor:  # Procesa dos archivos simultáneamente\n",
        "    for partial_results in tqdm(executor.map(process_jsonl, jsonl_files), total=len(jsonl_files)):\n",
        "        for key, value in partial_results.items():\n",
        "            if key in final_results:\n",
        "                final_results[key].extend(value)  # Evita sobrescribir, combina embeddings\n",
        "            else:\n",
        "                final_results[key] = value\n",
        "\n",
        "# --- Asignar los embeddings en el DataFrame ---\n",
        "for idx, row in val_small.iterrows():\n",
        "    updated_evidence = []\n",
        "    for ev in row[\"evidence\"]:\n",
        "        if isinstance(ev, list) and len(ev) >= 3:\n",
        "            ev_id = normalize_text(ev[2])\n",
        "\n",
        "            # Si la evidencia tiene \"None\" en los campos importantes, se mantiene igual\n",
        "            if ev_id == \"None\":\n",
        "                updated_evidence.append(ev)\n",
        "                continue\n",
        "\n",
        "            if ev_id in final_results:\n",
        "              for stored_ev in final_results[ev_id]:\n",
        "                  if stored_ev[:3] == ev[:3]:  # Verifica que sean exactamente la misma evidencia\n",
        "                      updated_evidence.append(stored_ev)\n",
        "                      break  # Como ya encontramos la evidencia correcta, no necesitamos seguir buscando\n",
        "\n",
        "            else:\n",
        "                updated_evidence.append(ev)  # mantiene la evidencia sin cambios si no tiene embedding\n",
        "\n",
        "    val_small.at[idx, \"evidence\"] = updated_evidence  # Modifica la fila directamente\n",
        "\n",
        "# --- Guardar en JSONL ---\n",
        "output_file = \"e_val_LoRA_full.jsonl\"\n",
        "\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in val_small.iterrows():\n",
        "        output_data = {}\n",
        "\n",
        "        for field in [\"verifiable\", \"label\", \"claim\", \"claim_embedding\"]:\n",
        "            if field in row:\n",
        "                output_data[field] = row[field]\n",
        "\n",
        "        output_data[\"evidence\"] = []\n",
        "        for ev in row[\"evidence\"]:\n",
        "            if isinstance(ev, list) and len(ev) > 3:\n",
        "                embedding = ev[-1] if isinstance(ev[-1], list) else []\n",
        "                output_data[\"evidence\"].append(embedding)\n",
        "\n",
        "        # Si todas las evidencias son listas vacías, deja solo una lista vacía global\n",
        "        if all(isinstance(ev, list) and not ev for ev in output_data[\"evidence\"]):\n",
        "            output_data[\"evidence\"] = []\n",
        "\n",
        "        f.write(ujson.dumps(output_data) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Archivo guardado con formato limpio: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jfj_q3-5Xoy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741863952425,
          "user_tz": -60,
          "elapsed": 387546,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "5062df4b-a778-4601-d606-e54508b78df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [06:18<00:00,  3.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo guardado con formato limpio: e_val_LoRA_full.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> División del e_val_LoRA en `val` y `test`: 10.000 muestras para el development set y 9.998 para el test set"
      ],
      "metadata": {
        "id": "ijD9ygzHpP8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = 10000\n",
        "\n",
        "val_small = pd.read_json(\"e_val_LoRA_full.jsonl\", lines=True)\n",
        "\n",
        "val_split = val_small.iloc[:split_index]\n",
        "test_split = val_small.iloc[split_index:]\n",
        "\n",
        "val_output_file = \"e_val_LoRA_split.jsonl\"\n",
        "test_output_file = \"e_test_LoRA_split.jsonl\"\n",
        "\n",
        "with open(val_output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in val_split.iterrows():\n",
        "        f.write(ujson.dumps(row.to_dict()) + \"\\n\")\n",
        "\n",
        "with open(test_output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in test_split.iterrows():\n",
        "        f.write(ujson.dumps(row.to_dict()) + \"\\n\")\n",
        "\n",
        "print(f\"✅ División completada. Archivos guardados como {val_output_file} y {test_output_file}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1AL42w6SUST",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741865735889,
          "user_tz": -60,
          "elapsed": 9333,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "c8927388-b655-40ee-ee23-b430931369dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ División completada. Archivos guardados como e_val_LoRA_split.jsonl y e_test_LoRA_split.jsonl.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*e_val_LoRA_split se almacena en HuggingFace con 10k, mientra que e_test_LoRA_split tiene otras 10k. Tambien se incluye el e_val_LoRA_full con la suma de ambos, que sería 20k*"
      ],
      "metadata": {
        "id": "UXULVcBrJ3cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2ª Fase: Combinación de los embeddings de los claims y evidencias junto con los detractores**\n",
        "\n",
        "En esta fase, existen multitud de formas para enlazar los artículos no relevantes a los claims. Entre ellas:\n",
        "\n",
        "1. **Añadir documentos no relevantes de manera random a cada claim**\n"
      ],
      "metadata": {
        "id": "wkPCME23TRsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import ujson\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- Parámetros ---\n",
        "repo_id = \"JORGEDC01/Demo1\"\n",
        "subfolder = \"wiki-pages/Embeddings_WikiPages/bge_small_en_v1.5/jsonl/\"\n",
        "jsonl_files = [f\"wiki-{i:03}.jsonl\" for i in range(1, 101)]  # Archivos de 001 a 100\n",
        "\n",
        "distractores_por_claim = 3\n",
        "file_path = \"/content/e_train_LoRA_50k.jsonl\"  # O \"/content/e_val_lora.jsonl\"\n",
        "output_file = \"e_train_distractors_LoRA.jsonl\"  # O \"e_val_distractors_LoRA.jsonl\"\n",
        "\n",
        "# --- Cargar el DataFrame con evidencias ya presentes ---\n",
        "df = pd.read_json(file_path, lines=True)\n",
        "\n",
        "max_distractors = len(df) * distractores_por_claim\n",
        "distractor_count = 0\n",
        "\n",
        "print(f\"🔹 Se asignarán un máximo de {max_distractors} distractores.\")\n",
        "\n",
        "distractors = []\n",
        "\n",
        "distractores_por_archivo = max_distractors // len(jsonl_files)\n",
        "residuo = max_distractors % len(jsonl_files)\n",
        "\n",
        "distractores_por_archivo_lista = [distractores_por_archivo] * len(jsonl_files)\n",
        "\n",
        "if residuo > 0:\n",
        "    archivos_con_residuo = random.sample(range(len(jsonl_files)), residuo)\n",
        "    for idx in archivos_con_residuo:\n",
        "        distractores_por_archivo_lista[idx] += 1\n",
        "\n",
        "def obtener_distractores(file_name, distractor_per_file):\n",
        "    global distractor_count\n",
        "\n",
        "    try:\n",
        "        file_path = hf_hub_download(repo_id=repo_id, filename=f\"{subfolder}{file_name}\", repo_type='dataset')\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                if distractor_count >= max_distractors:\n",
        "                    break\n",
        "                data = ujson.loads(line)\n",
        "                distractors.append(data[\"text_embedding\"])\n",
        "                distractor_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error procesando {file_name}: {e}\")\n",
        "\n",
        "\n",
        "progress_bar = tqdm(total=len(jsonl_files), desc=\"📥 Obteniendo distractores\")\n",
        "\n",
        "for idx, file in enumerate(jsonl_files):\n",
        "    distractor_per_file = distractores_por_archivo_lista[idx]\n",
        "    obtener_distractores(file, distractor_per_file)\n",
        "    progress_bar.update(1)\n",
        "\n",
        "progress_bar.close()\n",
        "\n",
        "random.shuffle(distractors)\n",
        "df[\"distractors\"] = [[] for _ in range(len(df))]\n",
        "\n",
        "for idx in range(len(df)):\n",
        "    selected_distractors = distractors[:distractores_por_claim]\n",
        "    distractors = distractors[distractores_por_claim:]\n",
        "    df.at[idx, \"distractors\"] = selected_distractors\n",
        "\n",
        "# --- Guardar en JSONL ---\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in df.iterrows():\n",
        "        f.write(ujson.dumps(row.to_dict()) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Archivo guardado como {output_file}.\")"
      ],
      "metadata": {
        "id": "8E-5674XECFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Completo**"
      ],
      "metadata": {
        "id": "25OOiPeSH1pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Método de elección *random* distribuido en las wiki-pages"
      ],
      "metadata": {
        "id": "ncEqhiwNQO7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import list_datasets\n",
        "\n",
        "datasets = list_datasets(author=\"JORGEDC01\")\n",
        "print([d.id for d in datasets])\n",
        "\n",
        "data_files = {\"train\": \"e_train.jsonl\", \"val\": \"e_val.jsonl\"}\n",
        "download_config = DownloadConfig(disable_tqdm=True)\n",
        "dataset = load_dataset(\"JORGEDC01/Demo1\", data_dir=\"Data/Embeddings_Data/bge_small_en_v1.5\", data_files=data_files, download_config=download_config) # oontiene los datos de claims de FEVER\n",
        "print(dataset)\n",
        "\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "df_val = pd.DataFrame(dataset['val'])\n",
        "\n",
        "del dataset\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533,
          "referenced_widgets": [
            "d53c61e7df8448d789df9bdb9e01c431",
            "7fdf2e4a3f0c4ba8843faefcab38fa11",
            "47cabf67fb7b4518a793d27de1e08a03",
            "2c7b0e613a2e49d6a5c201bbaad1fd17",
            "76fcf45d780549bb952342c1281119cd",
            "c2d62184dabd44c39e24708e6b3c0895",
            "4b744352d4624a889bcf1ec1a4a3c896",
            "bb1e10882a75410180c66378f4963374",
            "b53d7fa78a0c45d0ae4422245f95c1d3",
            "4b12c0c1e46846e69e602e7a9c98613a",
            "ac18d6471c0f44f6b9ac4a4000d64a7a",
            "021e0c4b66ef43ce8cdfb079cd560017",
            "c5cadd53aac041acb6aca4d32302d5be",
            "40cd42291e3a4f9ea8cb2a73989f163e",
            "6139f542582b4df0894515285f1d3f6e",
            "d06ddee391144c41b9052601f32a8011",
            "1848be6098384b158cd66bb53e265552",
            "a9323c3c551b4c78973bc752b70031f3",
            "0a8c76684e7f4251879f45c68d9ce382",
            "3b17cd69bd2f4bedbb7477e35d7c67a0",
            "8866632dcb1441de86bd77399840036d",
            "5b66c01bc66e4bddba491b10a6ba3322",
            "98c47e4b63d249d6a0530108c1347f61",
            "4e18e4461b4845b193766053f686d5a6",
            "3a6f1f20e75643be9eda4e5614e2d47f",
            "c66321ab691b483d8ec818b439d384ae",
            "20c64493229148fca9328a43c9f6a9d0",
            "f1c9ac5e07194101aaeb6c4a454cca22",
            "c39553f8fcbc46eda7a3d8b48643e0a8",
            "9d4df4f5c7974c6a85994889963f1224",
            "2f261c0b5e6b4641b6b58c8cbd40d618",
            "1572281a3a124d2e99e11b400c47b073",
            "c3d73adf07b24e25a6644b4ebf4912d1",
            "3edc9b6d7d8c4a7f9e4a6b75e67dbdf2",
            "6b2a1fadb7a14a4d8db559265cb0ad5d",
            "0048229812784e0088aa95b77a3749a6",
            "a2fc9eacadc94f32a6d8e35b1636c5c3",
            "dff52a54ceb94b099b95b75d9f4d0b5d",
            "9e55515927d9411486684bdcd06a5f0a",
            "4c50ee9bd94e4d9fb6bb12de13b28fbc",
            "232c7ca9845b4645917579a12ff591b7",
            "fa06367702644ea58cbc48ae86b7a670",
            "a1c9c070c1a04e8ea0570186f7c1da26",
            "efdb90aae2fa45b6b2f31a2aae49bfd7",
            "35ab5db5d671460ab0d0755c6724570d",
            "307c5dce90a44442ba40ed0e7ba4f074",
            "105c30c063fc48eea62d68f60c8243b3",
            "0dca552342364bce9a780289911c6b4a",
            "b7075fe2b26e40619cdbd42747005d9b",
            "90b2743f6bfd4be797069a4584d4f423",
            "f8efe4f6f533406494dc8947efe587bb",
            "7389190f4e6d42b18647d56a4cb01ce0",
            "aff9a3117c40493cba0fccac686a5ed8",
            "bae6dbdcca764ae28c484ae97abbb81d",
            "8eb89412b0004515be9bf9d14eeb9e4e"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741888024958,
          "user_tz": -60,
          "elapsed": 125544,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "99a753c1-b018-4219-d588-a18c425bafeb",
        "id": "i15tHGLiIzNE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['JORGEDC01/Demo1']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d53c61e7df8448d789df9bdb9e01c431"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "e_train.jsonl:   0%|          | 0.00/777M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "021e0c4b66ef43ce8cdfb079cd560017"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "e_val.jsonl:   0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98c47e4b63d249d6a0530108c1347f61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3edc9b6d7d8c4a7f9e4a6b75e67dbdf2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating val split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35ab5db5d671460ab0d0755c6724570d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'verifiable', 'label', 'claim', 'evidence', 'claim_embedding'],\n",
            "        num_rows: 145449\n",
            "    })\n",
            "    val: Dataset({\n",
            "        features: ['id', 'verifiable', 'label', 'claim', 'evidence', 'claim_embedding'],\n",
            "        num_rows: 19998\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_small, _ = train_test_split(df_train, train_size=0.35, stratify=df_train[\"label\"], random_state=42).copy()\n",
        "val_small = df_val.copy()\n",
        "\n",
        "print(f\"Tamaño original train: {df_train.shape}, Tamaño reducido: {train_small.shape}\")\n",
        "print(f\"Tamaño original val: {df_val.shape}, Tamaño reducido: {val_small.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741889498284,
          "user_tz": -60,
          "elapsed": 308,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "7109af61-b090-4780-f006-88c846761da8",
        "id": "KUQiSYmmIzNG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño original train: (145449, 6), Tamaño reducido: (50907, 6)\n",
            "Tamaño original val: (19998, 6), Tamaño reducido: (19998, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import ujson\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "import random\n",
        "\n",
        "# --- Hugging Face ---\n",
        "repo_id = \"JORGEDC01/Demo1\"\n",
        "subfolder = \"wiki-pages/Embeddings_WikiPages/bge_small_en_v1.5/jsonl/\"\n",
        "jsonl_files = [f\"wiki-{i:03}.jsonl\" for i in range(1, 101)]  # Desde 001 hasta 100\n",
        "\n",
        "distractores_por_claim = 3\n",
        "max_distractors = len(train_small) * distractores_por_claim\n",
        "distractor_count = 0\n",
        "\n",
        "print(f\"Se asignarán un máximo de {max_distractors} distractores.\")\n",
        "\n",
        "# --- Indice/Diccionario de evidencias ---\n",
        "evidence_dict = {}\n",
        "for _, row in train_small.iterrows():\n",
        "    for ev in row[\"evidence\"]:\n",
        "        if isinstance(ev, list) and len(ev) >= 3:\n",
        "            #ev_id = ev[2]  # ID de la evidencia\n",
        "            ev_id = normalize_text(ev[2])\n",
        "\n",
        "            if ev_id == \"None\":\n",
        "                continue\n",
        "\n",
        "            if ev_id not in evidence_dict:\n",
        "                evidence_dict[ev_id] = []\n",
        "            evidence_dict[ev_id].append(ev)\n",
        "\n",
        "distractors = []\n",
        "\n",
        "# --- Distribución de distractores entre los archivos JSONL ---\n",
        "distractores_por_archivo = max_distractors // len(jsonl_files)  # Parte entera de la división\n",
        "residuo = max_distractors % len(jsonl_files)  # Residuo, lo que sobra después de la división\n",
        "\n",
        "# Crear una lista para almacenar cuántos distractores se asignan a cada archivo\n",
        "distractores_por_archivo_lista = [distractores_por_archivo] * len(jsonl_files)\n",
        "\n",
        "# Si hay residuo, distribuirlo aleatoriamente entre los archivos\n",
        "if residuo > 0:\n",
        "    archivos_con_residuo = random.sample(range(len(jsonl_files)), residuo)  # Escoge aleatoriamente archivos\n",
        "    for idx in archivos_con_residuo:\n",
        "        distractores_por_archivo_lista[idx] += 1  # Asigna 1 distractor adicional a estos archivos\n",
        "\n",
        "# --- Función para procesar cada archivo JSONL ---\n",
        "def process_jsonl(file_name, distractor_per_file):\n",
        "    global distractor_count\n",
        "    results = {}\n",
        "\n",
        "    try:\n",
        "        file_path = hf_hub_download(repo_id=repo_id, filename=f\"{subfolder}{file_name}\", repo_type='dataset')\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                data = ujson.loads(line)\n",
        "                #wiki_id = data[\"id\"]\n",
        "                wiki_id = normalize_text(data[\"id\"])\n",
        "\n",
        "                if wiki_id in evidence_dict:\n",
        "                    for ev in evidence_dict[wiki_id]:\n",
        "                        ev.append(data[\"text_embedding\"])  # Se añade el embedding en el índice\n",
        "\n",
        "                    results[wiki_id] = evidence_dict[wiki_id]\n",
        "\n",
        "                else:\n",
        "                    if distractor_count < max_distractors:\n",
        "                        distractors.append(data[\"text_embedding\"])\n",
        "                        distractor_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {file_name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Procesar todos los archivos secuencialmente ---\n",
        "final_results = {}\n",
        "\n",
        "progress_bar = tqdm(total=len(jsonl_files), desc=\"Procesando archivos JSONL\")\n",
        "\n",
        "for idx, file in enumerate(jsonl_files):\n",
        "    distractor_per_file = distractores_por_archivo_lista[idx]  # Obtener el número de distractores para este archivo\n",
        "    partial_results = process_jsonl(file, distractor_per_file)\n",
        "\n",
        "    for key, value in partial_results.items():\n",
        "        if key in final_results:\n",
        "            final_results[key].extend(value)  # Evita sobrescribir, combina embeddings\n",
        "        else:\n",
        "            final_results[key] = value\n",
        "\n",
        "    progress_bar.update(1)  # Actualiza por cada archivo procesado\n",
        "\n",
        "progress_bar.close()\n",
        "\n",
        "random.shuffle(distractors)\n",
        "train_small[\"distractors\"] = [[] for _ in range(len(train_small))]\n",
        "\n",
        "# --- Asigna los embeddings en el DataFrame ---\n",
        "for idx, row in train_small.iterrows():\n",
        "    updated_evidence = []\n",
        "    for ev in row[\"evidence\"]:\n",
        "        if isinstance(ev, list) and len(ev) >= 3:\n",
        "            #ev_id = ev[2]\n",
        "            ev_id = normalize_text(ev[2])\n",
        "            # Si la evidencia tiene \"None\" en los campos importantes, se mantiene igual\n",
        "            if ev_id == \"None\":\n",
        "                updated_evidence.append(ev)\n",
        "                continue\n",
        "\n",
        "            # if ev_id in final_results:\n",
        "            #     updated_evidence.extend(final_results[ev_id])\n",
        "\n",
        "            if ev_id in final_results:\n",
        "              for stored_ev in final_results[ev_id]:\n",
        "                  if stored_ev[:3] == ev[:3]:  # Verifica que sean exactamente la misma evidencia\n",
        "                      updated_evidence.append(stored_ev)\n",
        "                      break  # Como ya encontramos la evidencia correcta, no necesitamos seguir buscando\n",
        "            else:\n",
        "                updated_evidence.append(ev)  # mantiene la evidencia sin cambios si no tiene embedding\n",
        "\n",
        "    # --- DISTRACTORES ---\n",
        "    selected_distractors = distractors[:distractores_por_claim].copy()\n",
        "    distractors = distractors[distractores_por_claim:].copy()\n",
        "\n",
        "    # Asignar los distractores a la fila\n",
        "    train_small.at[idx, \"distractors\"] = selected_distractors\n",
        "    train_small.at[idx, \"evidence\"] = updated_evidence\n",
        "\n",
        "\n",
        "# --- Guardar en JSONL con formato limpio ---\n",
        "output_file = \"e_train_distractors_LoRA_50k.jsonl\"\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in train_small.iterrows():\n",
        "        output_data = {}\n",
        "\n",
        "        # Mantener los campos clave del DataFrame\n",
        "        for field in [\"verifiable\", \"label\", \"claim\", \"claim_embedding\"]:\n",
        "            if field in row:\n",
        "                output_data[field] = row[field]\n",
        "\n",
        "        # Procesar las evidencias, guardando solo los embeddings finales\n",
        "        output_data[\"evidence\"] = []\n",
        "        for ev in row[\"evidence\"]:\n",
        "            if isinstance(ev, list) and len(ev) > 3:\n",
        "                embedding = ev[-1] if isinstance(ev[-1], list) else []\n",
        "                output_data[\"evidence\"].append(embedding)\n",
        "\n",
        "        # Añadir los distractores (si existen)\n",
        "        output_data[\"distractors\"] = row[\"distractors\"]\n",
        "\n",
        "        # Si todas las evidencias son listas vacías, deja solo una lista vacía global\n",
        "        if all(isinstance(ev, list) and not ev for ev in output_data[\"evidence\"]):\n",
        "            output_data[\"evidence\"] = []\n",
        "\n",
        "        f.write(ujson.dumps(output_data) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Archivo guardado con formato limpio: {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zWe3MzG9DgD",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741870686344,
          "user_tz": -60,
          "elapsed": 992283,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "b938f730-71c5-4f4f-a20d-890db7a46294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se asignarán un máximo de 152721 distractores.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando archivos JSONL: 100%|██████████| 100/100 [06:59<00:00,  4.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo guardado con formato limpio: e_train_distractors_LoRA_50k.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Development**"
      ],
      "metadata": {
        "id": "D1KDxNuZkDHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import ujson\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "import random\n",
        "import unicodedata\n",
        "\n",
        "# --- Hugging Face ---\n",
        "repo_id = \"JORGEDC01/Demo1\"\n",
        "subfolder = \"wiki-pages/Embeddings_WikiPages/bge_small_en_v1.5/jsonl/\"\n",
        "jsonl_files = [f\"wiki-{i:03}.jsonl\" for i in range(1, 101)]  # Desde 001 hasta 100\n",
        "\n",
        "distractores_por_claim = 3\n",
        "max_distractors = len(val_small) * distractores_por_claim\n",
        "distractor_count = 0\n",
        "\n",
        "print(f\"Se asignarán un máximo de {max_distractors} distractores.\")\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    return unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "\n",
        "# --- Indice/Diccionario de evidencias ---\n",
        "evidence_dict = {}\n",
        "for _, row in val_small.iterrows():\n",
        "    for ev in row[\"evidence\"]:\n",
        "        if isinstance(ev, list) and len(ev) >= 3:\n",
        "            #ev_id = ev[2]  # ID de la evidencia\n",
        "            ev_id = normalize_text(ev[2])\n",
        "\n",
        "            if ev_id == \"None\":\n",
        "                continue\n",
        "\n",
        "            if ev_id not in evidence_dict:\n",
        "                evidence_dict[ev_id] = []\n",
        "            evidence_dict[ev_id].append(ev)\n",
        "\n",
        "distractors = []\n",
        "\n",
        "# --- Distribución de distractores entre los archivos JSONL ---\n",
        "distractores_por_archivo = max_distractors // len(jsonl_files)  # Parte entera de la división\n",
        "residuo = max_distractors % len(jsonl_files)  # Residuo, lo que sobra después de la división\n",
        "\n",
        "# Crear una lista para almacenar cuántos distractores se asignan a cada archivo\n",
        "distractores_por_archivo_lista = [distractores_por_archivo] * len(jsonl_files)\n",
        "\n",
        "# Si hay residuo, distribuirlo aleatoriamente entre los archivos\n",
        "if residuo > 0:\n",
        "    archivos_con_residuo = random.sample(range(len(jsonl_files)), residuo)  # Escoge aleatoriamente archivos\n",
        "    for idx in archivos_con_residuo:\n",
        "        distractores_por_archivo_lista[idx] += 1  # Asigna 1 distractor adicional a estos archivos\n",
        "\n",
        "# --- Función para procesar cada archivo JSONL ---\n",
        "def process_jsonl(file_name, distractor_per_file):\n",
        "    global distractor_count\n",
        "    results = {}\n",
        "\n",
        "    try:\n",
        "        file_path = hf_hub_download(repo_id=repo_id, filename=f\"{subfolder}{file_name}\", repo_type='dataset')\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                data = ujson.loads(line)\n",
        "                #wiki_id = data[\"id\"]\n",
        "                wiki_id = normalize_text(data[\"id\"])\n",
        "\n",
        "                if wiki_id in evidence_dict:\n",
        "                    for ev in evidence_dict[wiki_id]:\n",
        "                        ev.append(data[\"text_embedding\"])  # Se añade el embedding en el índice\n",
        "                    results[wiki_id] = evidence_dict[wiki_id]\n",
        "\n",
        "                else:\n",
        "                    if distractor_count < max_distractors:\n",
        "                        distractors.append(data[\"text_embedding\"])\n",
        "                        distractor_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {file_name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Procesar todos los archivos secuencialmente ---\n",
        "final_results = {}\n",
        "\n",
        "progress_bar = tqdm(total=len(jsonl_files), desc=\"Procesando archivos JSONL\")\n",
        "\n",
        "for idx, file in enumerate(jsonl_files):\n",
        "    distractor_per_file = distractores_por_archivo_lista[idx]  # Obtener el número de distractores para este archivo\n",
        "    partial_results = process_jsonl(file, distractor_per_file)\n",
        "\n",
        "    for key, value in partial_results.items():\n",
        "        if key in final_results:\n",
        "            final_results[key].extend(value)  # Evita sobrescribir, combina embeddings\n",
        "        else:\n",
        "            final_results[key] = value\n",
        "\n",
        "    progress_bar.update(1)  # Actualiza por cada archivo procesado\n",
        "\n",
        "progress_bar.close()\n",
        "\n",
        "random.shuffle(distractors)\n",
        "val_small[\"distractors\"] = [[] for _ in range(len(val_small))]\n",
        "\n",
        "# --- Asigna los embeddings en el DataFrame ---\n",
        "for idx, row in val_small.iterrows():\n",
        "    updated_evidence = []\n",
        "    for ev in row[\"evidence\"]:\n",
        "        if isinstance(ev, list) and len(ev) >= 3:\n",
        "            #ev_id = ev[2]\n",
        "            ev_id = normalize_text(ev[2])\n",
        "\n",
        "            # Si la evidencia tiene \"None\" en los campos importantes, se mantiene igual\n",
        "            if ev_id == \"None\":\n",
        "                updated_evidence.append(ev)\n",
        "                continue\n",
        "\n",
        "            if ev_id in final_results:\n",
        "              for stored_ev in final_results[ev_id]:\n",
        "                  if stored_ev[:3] == ev[:3]:  # Verifica que sean exactamente la misma evidencia\n",
        "                      updated_evidence.append(stored_ev)\n",
        "                      break  # Como ya encontramos la evidencia correcta, no necesitamos seguir buscando\n",
        "            else:\n",
        "                updated_evidence.append(ev)  # mantiene la evidencia sin cambios si no tiene embedding\n",
        "\n",
        "    # --- DISTRACTORES ---\n",
        "    selected_distractors = distractors[:distractores_por_claim].copy()\n",
        "    distractors = distractors[distractores_por_claim:].copy()\n",
        "\n",
        "    # Asignar los distractores a la fila\n",
        "    val_small.at[idx, \"distractors\"] = selected_distractors\n",
        "    val_small.at[idx, \"evidence\"] = updated_evidence\n",
        "\n",
        "\n",
        "# --- Guardar en JSONL con formato limpio ---\n",
        "output_file = \"e_val_distractors_LoRA_full.jsonl\"\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in val_small.iterrows():\n",
        "        output_data = {}\n",
        "\n",
        "        # Mantener los campos clave del DataFrame\n",
        "        for field in [\"verifiable\", \"label\", \"claim\", \"claim_embedding\"]:\n",
        "            if field in row:\n",
        "                output_data[field] = row[field]\n",
        "\n",
        "        # Procesar las evidencias, guardando solo los embeddings finales\n",
        "        output_data[\"evidence\"] = []\n",
        "        for ev in row[\"evidence\"]:\n",
        "            if isinstance(ev, list) and len(ev) > 3:\n",
        "                embedding = ev[-1] if isinstance(ev[-1], list) else []\n",
        "                output_data[\"evidence\"].append(embedding)\n",
        "\n",
        "        # Añadir los distractores (si existen)\n",
        "        output_data[\"distractors\"] = row[\"distractors\"]\n",
        "\n",
        "        # Si todas las evidencias son listas vacías, deja solo una lista vacía global\n",
        "        if all(isinstance(ev, list) and not ev for ev in output_data[\"evidence\"]):\n",
        "            output_data[\"evidence\"] = []\n",
        "\n",
        "        f.write(ujson.dumps(output_data) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Archivo guardado con formato limpio: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr-qynflI5bq",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741889957858,
          "user_tz": -60,
          "elapsed": 442567,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "0a72bdc0-a9b1-4091-999d-ebf67f62d8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se asignarán un máximo de 59994 distractores.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando archivos JSONL: 100%|██████████| 100/100 [06:43<00:00,  4.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo guardado con formato limpio: e_val_distractors_LoRA_full.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Test**"
      ],
      "metadata": {
        "id": "pEmWs3evMREi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = 10000\n",
        "\n",
        "val_small = pd.read_json(\"e_val_distractors_LoRA_full.jsonl\", lines=True)\n",
        "\n",
        "val_split = val_small.iloc[:split_index]\n",
        "test_split = val_small.iloc[split_index:]\n",
        "\n",
        "val_output_file = \"e_val_distractors_LoRA_split.jsonl\"\n",
        "test_output_file = \"e_test_distractors_LoRA_split.jsonl\"\n",
        "\n",
        "with open(val_output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in val_split.iterrows():\n",
        "        f.write(ujson.dumps(row.to_dict()) + \"\\n\")\n",
        "\n",
        "with open(test_output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in test_split.iterrows():\n",
        "        f.write(ujson.dumps(row.to_dict()) + \"\\n\")\n",
        "\n",
        "print(f\"✅ División completada. Archivos guardados como {val_output_file} y {test_output_file}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TORB3dJqMTMM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741890302543,
          "user_tz": -60,
          "elapsed": 29911,
          "user": {
            "displayName": "Jorge del Castillo Gómez",
            "userId": "03598089572284782268"
          }
        },
        "outputId": "6bb44218-b619-48d4-9683-bbce2764a5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ División completada. Archivos guardados como e_val_distractors_LoRA_split.jsonl y e_test_distractors_LoRA_split.jsonl.\n"
          ]
        }
      ]
    }
  ]
}